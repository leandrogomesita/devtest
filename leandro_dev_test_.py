# -*- coding: utf-8 -*-
"""leandro_dev_test_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1axYHN8QdjTLz-xOAPT-az5e765mm6mrd
"""

import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

def generate_random_tables(n):
    """Generate synthetic tables for n days, simulating elevator request counts per floor per hour."""
    tables = {}
    for day in range(1, n+1):
        np.random.seed(day)  # Set the seed for reproducibility
        data = pd.DataFrame(data=np.random.randint(1, 6, size=(10, 24)),
                            index=pd.Index(range(1, 11), name='Day {}'.format(day)),
                            columns=range(1, 25))
        tables[day] = data
    return tables

# Generate 100 tables for 100 days
tables = generate_random_tables(100)

# Display a table for day 1 to check the data structure
print(tables[1])

def transform_data_for_training(tables, window=10):
    """Transform data using a sliding window approach for neural network input."""
    X, Y = [], []
    # Iterate over the data with a sliding window
    for day in range(1, len(tables) - window):
        # Input window: from 'day' to 'day + window - 1'
        input_window = pd.concat([tables[d] for d in range(day, day + window)], axis=1)
        # Output: the following day after the window
        output = tables[day + window].values.flatten()
        X.append(input_window.values.flatten())
        Y.append(output)
    return np.array(X), np.array(Y)

# Transform the data
X, Y = transform_data_for_training(tables)

# Split data into training and testing sets
X_train, Y_train = X[:80], Y[:80]  # First 80 days for training
X_test, Y_test = X[80:], Y[80:]   # Remaining 20 days for testing

print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)


# Recreate and compile the model (assuming TensorFlow access)
model = Sequential([
    Dense(512, input_dim=2400, activation='relu'),  # Input layer and first hidden layer
    Dense(256, activation='relu'),  # Additional hidden layer
    Dense(240, activation='linear')  # Output layer
])
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model and capture the history
history = model.fit(X_train, Y_train, epochs=100, verbose=1, validation_data=(X_test, Y_test))

# Plot training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model Loss During Training')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Evaluate the model on test data
test_loss = model.evaluate(X_test, Y_test, verbose=1)
print(f"Loss on test data: {test_loss}")